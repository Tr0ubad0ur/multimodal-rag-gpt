{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3770dcf",
   "metadata": {},
   "source": [
    "Проверка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "471e6f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "882dd989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "/Users/troubadour/Documents/Study/Diploma/multimodal-rag-gpt/.venv/lib/python3.13/site-packages/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff3f8410bf6d41c6a853d6f527acf914",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce2ea360b9a4e57bf9ddeea5e65200c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ LLM:\n",
      "В старину, в одной из маленьких деревень, жил котик по имени Боб. Он был очень умным и любил учиться. Однажды, когда Бобу было 10 лет, его хозяин, старый дядя, начал учить его программированию. \n",
      "\n",
      "Бобу было интересно, но он не мог понять, что это такое. Он начал учиться, изучая коды и понимая, как они работают. Он научился создавать простые программы, которые могли делать что-то полезное. \n",
      "\n",
      "Бобу очень любил учиться и учился очень быстро. Он научился создавать программы, которые могли делать что-то полезное. Он научился создавать программы, которые могли делать что-то полезное. Он научился создавать программы, которые могли делать что-то полезное. Он научился создавать программы, которые могли делать что-то полезное. Он научился создавать программы, которые могли делать что-то полезное. Он научился создавать программы, которые могли делать что-то полезное. Он научился создавать программы, которые могли делать что-то полезное. Он научился создавать программы, которые могли делать что-то полезное. Он научился\n"
     ]
    }
   ],
   "source": [
    "from backend.core.llm import QwenVisionLLM, get_llm_response\n",
    "\n",
    "llm = QwenVisionLLM()\n",
    "\n",
    "prompt = 'Напиши короткий рассказ о коте, который учится программированию.'\n",
    "\n",
    "response = get_llm_response(prompt)\n",
    "print('Ответ LLM:')  # noqa\n",
    "print(response)  # noqa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ced363",
   "metadata": {},
   "source": [
    "Проверка RAG-поиска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c8a985",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Folder data/test_data does not exist. Skipping data enrichment.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Результаты поиска:\n",
      "[{'id': 4, 'score': 1.0, 'payload': {'text': 'иональный баланс.\\n\\nВывод: Избежать стресса полностью невозможно, но можно научиться управлять им. Ключ к этому — комбинация здоровых привычек, самоорганизации и заботы о себе. Маленькие шаги каждый день дают огромный эффект на долгосрочной перспективе.', 'source': '/Users/troubadour/Documents/Study/Diploma/multimodal-rag-gpt/data/test_data/document.txt'}}]\n"
     ]
    }
   ],
   "source": [
    "from backend.core.embeddings import text_embedding\n",
    "from backend.utils.config_handler import Config\n",
    "from backend.utils.qdrant_handler import QdrantHandler\n",
    "\n",
    "qdrant = QdrantHandler(\n",
    "    url='localhost:6333',\n",
    "    collection_name=Config.qdrant_text_collection,\n",
    "    vector_size=Config.text_vector_size,\n",
    ")\n",
    "\n",
    "qdrant.enrich_with_data(\n",
    "    folder=str(project_root / 'data' / 'test_data'), embed_type='text'\n",
    ")\n",
    "\n",
    "query_text = 'иональный баланс.\\n\\nВывод: Избежать стресса полностью невозможно, но можно научиться управлять им. Ключ к этому — комбинация здоровых привычек, самоорганизации и заботы о себе. Маленькие шаги каждый день дают огромный эффект на долгосрочной перспективе.'\n",
    "\n",
    "query_vector = text_embedding(query_text)\n",
    "\n",
    "results = qdrant.search(query_vector=query_vector)\n",
    "\n",
    "print('Результаты поиска:')  # noqa\n",
    "print(results)  # noqa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal-rag-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
